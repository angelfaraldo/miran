{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import madmom as mmm\n",
    "from miran import *\n",
    "from subprocess import call\n",
    "from time import time\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnlss = folderfiles('/Users/angel/Insync/Datasets/giantsteps/splits/nnls', ext='.nnls')\n",
    "truth = folderfiles('/Users/angel/Insync/Datasets/giantsteps/keys_v2', ext='.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EXTRACT THE MEAN CHROMA PROFILES OF EACH HYPERMEASURE, ASSIGNING THEM A LABEL ACCORDING TO THE GROUND TRUTH OF EACH FILE\n",
    "\n",
    "filename = []\n",
    "meanchroma = []\n",
    "gkey = []\n",
    "\n",
    "for f in nnlss:\n",
    "    sf = strip_filename(f)\n",
    "    \n",
    "    find_annotation = 0\n",
    "    \n",
    "    for ff in truth:\n",
    "        sff = strip_filename(ff)\n",
    "        \n",
    "        if sff == sf[:sf.rfind(' - ')]:\n",
    "            \n",
    "            find_annotation = 1\n",
    "            \n",
    "            with open(ff, 'r') as estkey:\n",
    "                gkey.append(estkey.read())\n",
    "                \n",
    "    if find_annotation == 0:\n",
    "        print(\"COULD NOT FIND ANNOTATION FOR {}\".format(sf))\n",
    "            \n",
    "    temp = pd.DataFrame(csv_to_numpy(f, index_col=True))\n",
    "    meanchroma.append(np.roll(temp.median(), -3)) # Shift it so that vector indexes correspond to pitch classes.\n",
    "    filename.append(sf)\n",
    "    \n",
    "# CREATE A PD DATAFRAME WITH THE RESULTS, AND SAVE IT TO DISK\n",
    "output_df = pd.DataFrame(meanchroma, index=filename)\n",
    "output_df['gkey'] = gkey\n",
    "df_to_excel(output_df, '/Users/angel/Desktop/nnnssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD A FILE WITH THE ANALYSIS DATA.\n",
    "output_df = pd.read_excel('/Users/angel/Desktop/nnls_analysis.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE THE PREVIOUSLY EXTRACTED DATA TO TRAIN THE NEIGHBOURS ALGORITHM\n",
    "n_neighbors = 15\n",
    "\n",
    "X = output_df.drop('gkey', axis=1)\n",
    "y = output_df.gkey\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf.fit(X.as_matrix(), y.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing /Users/angel/Desktop/297598 Funk Revenge - White 2005 (Original Mix).mp3 in 32.5485141277 seconds.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS FROM AUDIO!!!!!\n",
    "sa_path = '/Users/angel/Git/miran/scripts/sonic-annotator'\n",
    "ff = folderfiles('/Users/angel/Desktop/')\n",
    "\n",
    "idx = 0\n",
    "beat_tracker = mmm.features.beats.DBNDownBeatTrackingProcessor(beats_per_bar=16, fps=100, downbeats=True)\n",
    "\n",
    "for f in ff:\n",
    "    if any(soundfile_type in f for soundfile_type in AUDIO_FILE_EXTENSIONS):\n",
    "        \n",
    "        i = time()\n",
    "        print(\"Analysing {}\".format(f), end='')\n",
    "        \n",
    "        # TEMPORARILY RENAME FILE TO AVOID ILLEGAL CHARACTERS.\n",
    "        fname, fext = os.path.splitext(f)\n",
    "        fdir, fname = os.path.split(fname)\n",
    "        fname = os.path.join(fdir, str(idx))\n",
    "        f2 = fname + fext\n",
    "        os.rename(f, f2)\n",
    "\n",
    "        # INIT PLACEHOLDERS\n",
    "        file_df = pd.DataFrame()\n",
    "        key     = [np.nan]\n",
    "        idist   = [np.nan]\n",
    "\n",
    "        # DOWNBEAT COMPUTATION\n",
    "        beat_activations = mmm.features.beats.RNNDownBeatProcessor()(f2)\n",
    "        downbeat_positions = beat_tracker(beat_activations)\n",
    "        \n",
    "        # VAMP-PLUGIN ANALYSIS\n",
    "        call('{0}/sonic-annotator -r -t {0}/nnls.n3 -w csv --csv-force \"{1}\"'.format(sa_path, f2), shell=True)\n",
    "        nnls = pd.read_csv(os.path.splitext(f2)[0] + '_vamp_nnls-chroma_nnls-chroma_chroma.csv', header=None)\n",
    "        \n",
    "        # ADD ZERO POSTION\n",
    "        if 0 not in downbeat_positions:\n",
    "            downbeat_positions = np.append(downbeat_positions, 0.00)\n",
    "            downbeat_positions = np.roll(downbeat_positions, 1)\n",
    "\n",
    "        # MAKE CALCULATION BASED ON HYPERMETRICAL DIVISIONS\n",
    "        for position in range(len(downbeat_positions) - 1):\n",
    "            temp = nnls[nnls[0] >= downbeat_positions[position]]\n",
    "            temp = nnls[nnls[0] <  downbeat_positions[position + 1]]\n",
    "            mean_chroma = np.roll(temp.drop(0,1).median(), -3)\n",
    "            key.append(clf.predict([mean_chroma])[0])\n",
    "            dist = float(clf.kneighbors([mean_chroma],1)[0])\n",
    "            idist.append(1 / dist)\n",
    "            \n",
    "        # SAVE ALL RESULTS PER HYPERMEASURE TO PD DATAFRAME.\n",
    "        file_df['downbeats'] = downbeat_positions\n",
    "        file_df['key']       = np.roll(key, -1)\n",
    "        file_df['idist']     = np.roll(idist, -1)\n",
    "        \n",
    "        print(\" in {} seconds.\".format(time() - i))\n",
    "        \n",
    "        # REMOVE INTERMEDIATE FILES AND RENAME WITH ORIGINAL NAME\n",
    "        os.remove(os.path.splitext(f2)[0] + '_vamp_nnls-chroma_nnls-chroma_chroma.csv')\n",
    "        os.rename(f2, f)\n",
    "        idx += 1\n",
    "\n",
    "        # MAKE GLOBAL KEY ESTIMATION BASED ON HYPERMETRICAL KEYS\n",
    "        global_keys_candidates = []\n",
    "        global_keys_confidence = []\n",
    "        \n",
    "        # we add the distances of the closests candidates providing the same key\n",
    "        for item in file_df.key.unique():\n",
    "            if item != 'nan':\n",
    "                key_df = file_df[file_df.key == item]\n",
    "                global_keys_candidates.append(item)\n",
    "                global_keys_confidence.append(key_df.idist.sum())\n",
    "                \n",
    "        global_key = global_keys_candidates[global_keys_confidence.index(max(global_keys_confidence))]\n",
    "        \n",
    "        # SAVE SINGLE GLOBAL ESTIMATION TO TEXT FILE AND REMOVE INTERMEDIATE FILES\n",
    "        global_key_dir = '/Users/angel/Desktop/test_keys_audio'\n",
    "        with open(os.path.join(global_key_dir, os.path.split(os.path.splitext(f)[0])[1] + '.txt'), 'w') as textfile:\n",
    "                  textfile.write(global_key)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing /Users/angel/Desktop/297541 Mecanique - Silverface (Original Mix).nnls in 0.147009849548 seconds.\n",
      "Analysing /Users/angel/Desktop/297598 Funk Revenge - White 2005 (Original Mix).nnls in 0.0812511444092 seconds.\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS FROM FILES!!!\n",
    "\n",
    "sa_path = '/Users/angel/Git/miran/scripts/sonic-annotator'\n",
    "ff = folderfiles('/Users/angel/Desktop/')\n",
    "\n",
    "for f in ff:\n",
    "    fname, fext = os.path.splitext(f)\n",
    "    if fext == '.nnls':\n",
    "        if fname + '.loop' in ff:\n",
    "            i = time()\n",
    "            print(\"Analysing {}\".format(f), end='')\n",
    "\n",
    "            nnls = pd.read_csv(f, header=None)\n",
    "            downbeat_positions = pd.read_csv(fname + '.loop', header=None).as_matrix().flatten()\n",
    "\n",
    "            # INIT PLACEHOLDERS\n",
    "            file_df = pd.DataFrame()\n",
    "            key     = [np.nan]\n",
    "            idist   = [np.nan]\n",
    "\n",
    "            # ADD ZERO POSTION\n",
    "            if 0 not in downbeat_positions:\n",
    "                downbeat_positions = np.append(downbeat_positions, 0.00)\n",
    "                downbeat_positions = np.roll(downbeat_positions, 1)\n",
    "\n",
    "            # MAKE CALCULATION BASED ON HYPERMETRICAL DIVISIONS\n",
    "            for position in range(len(downbeat_positions) - 1):\n",
    "                temp = nnls[nnls[0] >= downbeat_positions[position]]\n",
    "                temp = nnls[nnls[0] <  downbeat_positions[position + 1]]\n",
    "                mean_chroma = np.roll(temp.drop(0,1).median(), -3)\n",
    "                key.append(clf.predict([mean_chroma])[0])\n",
    "                dist = float(clf.kneighbors([mean_chroma],1)[0])\n",
    "                idist.append(1 / dist)\n",
    "\n",
    "            # SAVE ALL RESULTS PER HYPERMEASURE TO PD DATAFRAME.\n",
    "            file_df['downbeats'] = downbeat_positions\n",
    "            file_df['key']       = np.roll(key, -1)\n",
    "            file_df['idist']     = np.roll(idist, -1)\n",
    "\n",
    "            print(\" in {} seconds.\".format(time() - i))\n",
    "\n",
    "            # MAKE GLOBAL KEY ESTIMATION BASED ON HYPERMETRICAL KEYS\n",
    "            global_keys_candidates = []\n",
    "            global_keys_confidence = []\n",
    "\n",
    "            # we add the distances of the closests candidates providing the same key\n",
    "            for item in file_df.key.unique():\n",
    "                if item != 'nan':\n",
    "                    key_df = file_df[file_df.key == item]\n",
    "                    global_keys_candidates.append(item)\n",
    "                    global_keys_confidence.append(key_df.idist.sum())\n",
    "\n",
    "            global_key = global_keys_candidates[global_keys_confidence.index(max(global_keys_confidence))]\n",
    "\n",
    "            # SAVE SINGLE GLOBAL ESTIMATION TO TEXT FILE AND REMOVE INTERMEDIATE FILES\n",
    "            global_key_dir = '/Users/angel/Desktop/test_keys_text'\n",
    "            with open(os.path.join(global_key_dir, os.path.split(os.path.splitext(f)[0])[1] + '.txt'), 'w') as textfile:\n",
    "                      textfile.write(global_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
